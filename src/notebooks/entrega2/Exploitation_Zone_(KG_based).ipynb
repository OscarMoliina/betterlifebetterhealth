{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploitation Zone (KG-based)**"
      ],
      "metadata": {
        "id": "IN2-9ntcwbUx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGPP1ZgLgu_A",
        "outputId": "81f31100-4395-46af-85c1-188d66ccb4ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'betterlifebetterhealth'...\n",
            "remote: Enumerating objects: 376, done.\u001b[K\n",
            "remote: Counting objects: 100% (221/221), done.\u001b[K\n",
            "remote: Compressing objects: 100% (186/186), done.\u001b[K\n",
            "remote: Total 376 (delta 64), reused 131 (delta 35), pack-reused 155\u001b[K\n",
            "Receiving objects: 100% (376/376), 100.74 MiB | 18.60 MiB/s, done.\n",
            "Resolving deltas: 100% (114/114), done.\n",
            "Updating files: 100% (45/45), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/OscarMoliina/betterlifebetterhealth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pyspark\n",
        "import pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col, mean, when, lit, count, to_json\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import mean\n",
        "from pyspark.sql.functions import mean\n",
        "from pyspark.sql.types import IntegerType, DoubleType\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import collect_list\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import duckdb"
      ],
      "metadata": {
        "id": "Gt1wtnMQhNUL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b939e4b5-20cf-4328-a0cc-b2a83a972f3d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=f15ea5881091376e1a970d81e8939bf135f39993f2f67dcd63545f63caab3161\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Preprocessing\") \\\n",
        "    .config(\"spark.jars\", \"/content/betterlifebetterhealth/src/utils/duckdb.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "join = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:duckdb:/content/betterlifebetterhealth/data/db/exploitation_zone.db\") \\\n",
        "    .option(\"driver\", \"org.duckdb.DuckDBDriver\") \\\n",
        "    .option(\"dbtable\", \"join_table\") \\\n",
        "    .load()"
      ],
      "metadata": {
        "id": "I1xphyO6hQ2Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Classificació dels països per regió:**\n",
        "Afegim relació per dividir Europa en 4 zones: Europa del Este, Europa del Sur, Europa Occidental y Europa del Norte.\n",
        "\n",
        "1. Europa del Nord: Suècia, Finlàndia, Noruega, Dinamarca, Islàndia, Estònia, Letònia, Lituània\n",
        "2. Europa de l'Est: Polònia, Eslovàquia, República Txeca, Hongria, Romania, Bulgària, Bielorússia, Ucraïna, Moldàvia, Rússia, Albània, Kosovo, Macedònia del Nord, Montenegro, Sèrbia, Bòsnia i Hercegovina\n",
        "3. Europa del Sud: Itàlia, Espanya, Portugal, Grècia, Turquia, Xipre, Malta, Croàcia, Eslovènia\n",
        "4. Europa Occidental: Alemanya, França, Bèlgica, Països Baixos, Àustria, Suïssa, Luxemburg, Regne Unit, Irlanda"
      ],
      "metadata": {
        "id": "OxSKEtDAlwqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de países por región incluyendo Asia\n",
        "north_europe = ['Sweden', 'Finland', 'Norway', 'Denmark', 'Iceland', 'Estonia', 'Latvia', 'Lithuania']\n",
        "east_europe = ['Poland', 'Slovakia', 'Hungary', 'Romania', 'Bulgaria', 'Belarus', 'Ukraine', 'Albania', 'Moldova', 'Czech Republic', 'Russia', 'North Macedonia', 'Montenegro', 'Serbia', 'Bosnia and Herzegovina']\n",
        "south_europe = ['Italy', 'Spain', 'Portugal', 'Greece', 'Cyprus', 'Malta', 'Croatia', 'Slovenia']\n",
        "west_europe = ['Germany', 'France', 'Belgium', 'Netherlands', 'Austria', 'Switzerland', 'Luxembourg', 'United Kingdom', 'Ireland']\n",
        "asia = ['Turkey', 'Israel', 'Georgia', 'Azerbaijan', 'Armenia', 'Kazakhstan', 'Uzbekistan', 'Turkmenistan', 'Tajikistan', 'Kyrgyzstan']\n",
        "\n",
        "# Función para asignar la región basada en el país\n",
        "def assign_region(country):\n",
        "    if country in north_europe:\n",
        "        return 'Europe North'\n",
        "    elif country in east_europe:\n",
        "        return 'Europe East'\n",
        "    elif country in south_europe:\n",
        "        return 'Europe South'\n",
        "    elif country in west_europe:\n",
        "        return 'Europe West'\n",
        "    elif country in asia:\n",
        "        return 'Asia'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "region_udf = udf(assign_region, StringType())\n",
        "\n",
        "# Crear una nueva columna con la región asignada\n",
        "join = join.withColumn('Region', region_udf(join['Country']))"
      ],
      "metadata": {
        "id": "L_cGB6sPl2Ps"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pertanyença a UE:**\n",
        "Afegim relació per distingir els que pertanyen a la Unió Europea i els que no."
      ],
      "metadata": {
        "id": "4PL8lsJVntCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eu_countries = [\n",
        "    'Austria', 'Belgium', 'Bulgaria', 'Croatia', 'Cyprus', 'Czech Republic', 'Denmark',\n",
        "    'Estonia', 'Finland', 'France', 'Germany', 'Greece', 'Hungary', 'Ireland', 'Italy',\n",
        "    'Latvia', 'Lithuania', 'Luxembourg', 'Malta', 'Netherlands', 'Poland', 'Portugal',\n",
        "    'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Sweden'\n",
        "]\n",
        "\n",
        "# Función para determinar la membresía en la UE\n",
        "def is_eu_member(country):\n",
        "    return 'EU Member' if country in eu_countries else 'Non-EU Member'\n",
        "\n",
        "# UDF para aplicar la función en Spark\n",
        "eu_member_udf = udf(is_eu_member, StringType())\n",
        "\n",
        "join = join.withColumn('EU Membership', eu_member_udf(join['Country']))"
      ],
      "metadata": {
        "id": "ILFbxnGDnytA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pertanyença a NATO:**\n",
        "\n"
      ],
      "metadata": {
        "id": "tR6_IeMaoW-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de países miembros de la OTAN\n",
        "nato_countries = [\n",
        "    'Albania', 'Belgium', 'Bulgaria', 'Canada', 'Croatia', 'Czech Republic', 'Denmark',\n",
        "    'Estonia', 'France', 'Germany', 'Greece', 'Hungary', 'Iceland', 'Italy', 'Latvia',\n",
        "    'Lithuania', 'Luxembourg', 'Montenegro', 'Netherlands', 'North Macedonia', 'Norway',\n",
        "    'Poland', 'Portugal', 'Romania', 'Slovakia', 'Slovenia', 'Spain', 'Turkey', 'United Kingdom', 'United States'\n",
        "]\n",
        "\n",
        "# Función para determinar la membresía en la OTAN\n",
        "def is_nato_member(country):\n",
        "    return 'NATO Member' if country in nato_countries else 'Non-NATO Member'\n",
        "\n",
        "# UDF para aplicar la función en Spark\n",
        "nato_member_udf = udf(is_nato_member, StringType())\n",
        "\n",
        "join = join.withColumn('NATO Membership', nato_member_udf(join['Country']))\n"
      ],
      "metadata": {
        "id": "6Fhrwg64oqMo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Països que fan frontera:**"
      ],
      "metadata": {
        "id": "IV3vfczmwyl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fronteres = spark.read.option(\"header\", \"true\").csv('/content/betterlifebetterhealth/data/csv/GEODATASOURCE-COUNTRY-BORDERS.CSV')"
      ],
      "metadata": {
        "id": "rbdZvhNPw9Oi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "borders_grouped = fronteres.groupBy(\"country_name\").agg(collect_list(\"country_border_name\").alias(\"border_countries\"))"
      ],
      "metadata": {
        "id": "pwvE1VBQyGPI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join = join.join(borders_grouped, join['Country'] == borders_grouped['country_name'], 'left')"
      ],
      "metadata": {
        "id": "G9zeuwyOza18"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join = join.drop(\"country_name\")"
      ],
      "metadata": {
        "id": "L5gPNxKg0ogr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join = join.withColumn(\"border_countries\", to_json(col(\"border_countries\")))"
      ],
      "metadata": {
        "id": "i08HTT2xG7rW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join.write \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:duckdb:exploitation_zone_2.db\") \\\n",
        "    .option(\"dbtable\", \"join_new_relations\") \\\n",
        "    .option(\"driver\", \"org.duckdb.DuckDBDriver\") \\\n",
        "    .save()\n"
      ],
      "metadata": {
        "id": "h1I_6XA4GAhZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "28529ddd-74d0-4df9-a6f1-323283bf80ba"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Table or view 'join_new_relations' already exists. SaveMode: ErrorIfExists.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-2bbdc468d722>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dbtable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"join_new_relations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"driver\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"org.duckdb.DuckDBDriver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1461\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Table or view 'join_new_relations' already exists. SaveMode: ErrorIfExists."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Immigration**"
      ],
      "metadata": {
        "id": "rUydv36t1f0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand, monotonically_increasing_id\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "countries_df = join.select(\"Country\").distinct()\n",
        "origins = countries_df.withColumnRenamed(\"Country\", \"Origin\")\n",
        "destinations = countries_df.withColumnRenamed(\"Country\", \"Destination\")\n",
        "immigration_df = origins.crossJoin(destinations)\n",
        "immigration_df = immigration_df.where(immigration_df[\"Origin\"] != immigration_df[\"Destination\"])\n",
        "immigration_df = immigration_df.withColumn(\"Immigrants\", (rand(seed=1234) * 10000).cast(\"integer\"))\n",
        "immigration_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QF7lq81T1sqq",
        "outputId": "9e47e125-eed3-4481-848d-f46513793e32"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+----------+\n",
            "| Origin| Destination|Immigrants|\n",
            "+-------+------------+----------+\n",
            "| Sweden|       Italy|      6262|\n",
            "| Sweden|Turkmenistan|      1841|\n",
            "| Sweden|     Denmark|      5914|\n",
            "| Sweden|      Israel|      4050|\n",
            "| Sweden|    Portugal|      1151|\n",
            "| Turkey|     Finland|       266|\n",
            "| Turkey|       Spain|      7422|\n",
            "| Turkey|      Poland|      9841|\n",
            "|Germany|       Spain|      1383|\n",
            "|Germany|     Georgia|      5130|\n",
            "|Germany|    Bulgaria|      7031|\n",
            "| France|     Belarus|      3461|\n",
            "| France|  Azerbaijan|      9103|\n",
            "| France|     Armenia|      9222|\n",
            "| France|      Latvia|      3869|\n",
            "| France|      Poland|      9254|\n",
            "| France|    Portugal|      9739|\n",
            "| Greece|    Slovakia|      2004|\n",
            "| Greece|     Albania|      8289|\n",
            "| Greece|      Israel|      3084|\n",
            "+-------+------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar el DataFrame de inmigración en la base de datos\n",
        "immigration_df.write \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:duckdb:exploitation_zone_2.db\") \\\n",
        "    .option(\"dbtable\", \"immigration_relations\") \\\n",
        "    .option(\"driver\", \"org.duckdb.DuckDBDriver\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .save()"
      ],
      "metadata": {
        "id": "6G2cITwO3Bb2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Guardar db a github:**"
      ],
      "metadata": {
        "id": "rO-Qne17HqzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository\n",
        "%cd betterlifebetterhealth/data/db\n",
        "\n",
        "# Copy new files from your local directory to this directory\n",
        "!cp -R /content/*.db ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCwuMYgVHtJX",
        "outputId": "0a9d68e2-b920-4e18-f26d-1dc6ac064754"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/betterlifebetterhealth/data/db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CODI A LA DOCUMENTACIO NO POSAT AQUÍ PER PRIVACITAT DE USUARI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS0Z2sCqIdbH",
        "outputId": "954a0c3b-7c86-44fa-f161-9d1d97e5fb24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 50cd8a3] Update db files new\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            " create mode 100644 data/db/exploitation_zone_2.db\n",
            "Enumerating objects: 8, done.\n",
            "Counting objects: 100% (8/8), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (5/5), done.\n",
            "Writing objects: 100% (5/5), 197.17 KiB | 5.19 MiB/s, done.\n",
            "Total 5 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/OscarMoliina/betterlifebetterhealth.git\n",
            "   1fa8450..50cd8a3  main -> main\n"
          ]
        }
      ]
    }
  ]
}