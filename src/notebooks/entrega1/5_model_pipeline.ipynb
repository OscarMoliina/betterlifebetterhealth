{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSHdUIux0nhn",
        "outputId": "6f504b01-4dcf-452e-cd62-5cbad742bcf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=4fe2d79365f93440b9ad3fcb61e4d9072641581edce93d04998f55b79ba923fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.10/dist-packages (0.10.2)\n"
          ]
        }
      ],
      "source": [
        "#!pip3 install scikit-learn\n",
        "!pip3 install xgboost\n",
        "!pip3 install pyspark\n",
        "!pip3 install duckdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "500kWtEo0nhp"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import duckdb\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/OscarMoliina/betterlifebetterhealth.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylH7mQ7W1fKE",
        "outputId": "83611e72-c537-43fc-d05e-9d88035d3e2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'betterlifebetterhealth'...\n",
            "remote: Enumerating objects: 332, done.\u001b[K\n",
            "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
            "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
            "remote: Total 332 (delta 56), reused 120 (delta 34), pack-reused 155\u001b[K\n",
            "Receiving objects: 100% (332/332), 100.30 MiB | 23.70 MiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2pvOvAUu0nhq"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Preprocessing\") \\\n",
        "    .config(\"spark.jars\", \"/content/betterlifebetterhealth/src/utils/duckdb.jar\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "data = spark.read \\\n",
        "    .format(\"jdbc\") \\\n",
        "    .option(\"url\", \"jdbc:duckdb:/content/betterlifebetterhealth/data/db/exploitation_zone.db\") \\\n",
        "    .option(\"driver\", \"org.duckdb.DuckDBDriver\") \\\n",
        "    .option(\"dbtable\", \"join_table\") \\\n",
        "    .load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgJmGYfq0nhq"
      },
      "source": [
        "# Comparativa Dataset Simple vs. Dataset Mitxe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "q-i8kmJp0nhr"
      },
      "outputs": [],
      "source": [
        "df = data.toPandas()\n",
        "\n",
        "y = df['Depression (%)']\n",
        "X1 = df[['Schizophrenia (%)', 'Bipolar disorder (%)', 'Eating disorders (%)',\n",
        "         'Anxiety disorders (%)', 'Drug use disorders (%)', 'Alcohol use disorders (%)']]\n",
        "X2 = df.drop(['Country', 'Year', 'Depression (%)'], axis=1)\n",
        "\n",
        "X1_train, X1_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=2003)\n",
        "X2_train, X2_test = train_test_split(X2, test_size=0.2, random_state=2003)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X1_train_scaled = scaler.fit_transform(X1_train)\n",
        "X1_test_scaled = scaler.transform(X1_test)\n",
        "X2_train_scaled = scaler.fit_transform(X2_train)\n",
        "X2_test_scaled = scaler.transform(X2_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZY4VsOz0nhs",
        "outputId": "f0c44f0f-f471-4892-a798-dfafb6abfdb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE para el modelo con solo datos de enfermedades: 0.003019290312440158\n",
            "MSE para el modelo con todas las columnas: 0.005844207276130293\n"
          ]
        }
      ],
      "source": [
        "svm_model1 = SVR()\n",
        "svm_model2 = SVR()\n",
        "svm_model1.fit(X1_train_scaled, y_train)\n",
        "svm_model2.fit(X2_train_scaled, y_train)\n",
        "\n",
        "# joblib.dump(scaler, 'scaler1.pkl')\n",
        "# joblib.dump(svm_model1, 'svm_model1.pkl')\n",
        "# joblib.dump(svm_model2, 'svm_model2.pkl')\n",
        "\n",
        "y_pred1 = svm_model1.predict(X1_test_scaled)\n",
        "y_pred2 = svm_model2.predict(X2_test_scaled)\n",
        "mse1 = mean_squared_error(y_test, y_pred1)\n",
        "mse2 = mean_squared_error(y_test, y_pred2)\n",
        "\n",
        "print(\"MSE para el modelo con solo datos de enfermedades:\", mse1)\n",
        "print(\"MSE para el modelo con todas las columnas:\", mse2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLa-0vXz0nhs"
      },
      "source": [
        "# Model Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MIzbVInq0nht"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "from pyspark.sql import DataFrame\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "class ModelPipeline:\n",
        "    def __init__(self, data: DataFrame, objective:str, model_type='SVM', model=None, scaler=None) -> None:\n",
        "        self.data = data\n",
        "        self.objective = objective\n",
        "        self.model_type = model_type\n",
        "        self.model = model\n",
        "        self.scaler = scaler if scaler is not None else StandardScaler()\n",
        "\n",
        "    def __train_model(self):\n",
        "        df = self.data.toPandas()\n",
        "        y = df[self.objective]\n",
        "        X = df.drop(['Country', 'Year', self.objective], axis=1)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2003)\n",
        "\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        if self.model_type == 'SVM':\n",
        "            self.model = SVR()\n",
        "        elif self.model_type == 'RF':\n",
        "            self.model = RandomForestRegressor()\n",
        "        elif self.model_type == 'XGB':\n",
        "            self.model = XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "        self.model.fit(X_train_scaled, y_train)\n",
        "        y_pred = self.model.predict(X_test_scaled)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "        joblib.dump(self.model, f'{self.model_type}_model.pkl')\n",
        "        joblib.dump(self.scaler, 'scaler.pkl')\n",
        "\n",
        "        return self.model, self.scaler, None, mse\n",
        "\n",
        "    def __make_predictions(self):\n",
        "        df = self.data.toPandas()\n",
        "        X = df.drop(['Country', 'Year', self.objective], axis=1)\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        y_pred = self.model.predict(X_scaled)\n",
        "\n",
        "        return self.model, self.scaler, y_pred, None\n",
        "\n",
        "    def predict(self):\n",
        "        if self.model is None or isinstance(self.model, str) and not self.model.endswith('.pkl'):\n",
        "            return self.__train_model()\n",
        "        else:\n",
        "            self.model = joblib.load(self.model) if isinstance(self.model, str) else self.model\n",
        "            self.scaler = joblib.load(self.scaler) if isinstance(self.scaler, str) else self.scaler\n",
        "            return self.__make_predictions()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1yR9zGi0nht"
      },
      "source": [
        "# Exemples d'Ús"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xf88MeZN0nhu"
      },
      "outputs": [],
      "source": [
        "pipline = ModelPipeline(data=data, objective='Depression (%)', model_type='XGB', scaler=None, model=None)\n",
        "model, scaler, predictions, mse = pipline.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taxeErV30nhu",
        "outputId": "1f0d8334-6d1d-4563-dcde-da4accb6d955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
            "             colsample_bylevel=None, colsample_bynode=None,\n",
            "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
            "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
            "             gamma=None, grow_policy=None, importance_type=None,\n",
            "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
            "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
            "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
            "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
            "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
            "             num_parallel_tree=None, random_state=None, ...)\n",
            "StandardScaler()\n",
            "None\n",
            "6.929342554497845e-05\n"
          ]
        }
      ],
      "source": [
        "print(model)\n",
        "print(scaler)\n",
        "print(predictions)\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OHea_3qP0nhv"
      },
      "outputs": [],
      "source": [
        "pipline = ModelPipeline(data=data, objective='Depression (%)',model_type='SVM')\n",
        "model, scaler, predictions, mse = pipline.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onmRa7sJ0nhv",
        "outputId": "d8d9df7a-adbd-4538-e0f1-924e58b7eb66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVR()\n",
            "StandardScaler()\n",
            "None\n",
            "0.005844207276130293\n"
          ]
        }
      ],
      "source": [
        "print(model)\n",
        "print(scaler)\n",
        "print(predictions)\n",
        "print(mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xq7CZ2Ng0nhv"
      },
      "outputs": [],
      "source": [
        "pipline = ModelPipeline(data=data, objective='Depression (%)',model_type='RF')\n",
        "model, scaler, predictions, mse = pipline.predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwRViAoy0nhw",
        "outputId": "9978a890-f5ac-4b76-9819-9bc3a186943e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestRegressor()\n",
            "StandardScaler()\n",
            "None\n",
            "5.399841463131409e-05\n"
          ]
        }
      ],
      "source": [
        "print(model)\n",
        "print(scaler)\n",
        "print(predictions)\n",
        "print(mse)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}